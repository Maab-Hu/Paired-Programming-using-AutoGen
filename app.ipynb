{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import autogen\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_list = [\n",
    "    {\n",
    "        'model': 'gpt-3.5-turbo-0613',\n",
    "        'api_key': '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'\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_config = {\n",
    "    \"use_cache\": False,\n",
    "    \"request_timeout\":600,\n",
    "    \"config_list\":config_list,\n",
    "    \"temperature\":0.7\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_master = autogen.AssistantAgent(\n",
    "    human_input_mode=\"NEVER\",\n",
    "    name=\"taskmaster\",\n",
    "    llm_config= llm_config,\n",
    "     max_consecutive_auto_reply=10,\n",
    "    system_message = \"\"\" \n",
    "    You are a Task Master AI that will provide amateur level Machine Learning Task for Python to the Junior Bot Assistant Agent. \n",
    "    You should not provide solutions or in depth instructions or hints. If the task requires a dataset \\\n",
    "    make a simple dataset and store it in .csv format for the user to work on. If the tasks requires working on already declared objects or variables, specify \\\n",
    "    the name of the variable and objects and clarify that those declarations have been made. Provide tasks that require 1 or 2 lines of code.\n",
    "    \n",
    "    When the Junior Bot or the user provides the answer for the tasks, provide feedback on if it is correct or not, but never provide any code. If the user answers in natural language, ask for code solution. \n",
    "    If the user provides the correct solution , move on and provide another task , otherwise keep providing feedback till the user fixes their code. Each task provided must be distinct. \n",
    "    \n",
    "    Only provide Machine Learning related tasks, only provide tasks to the Junior Bot. \n",
    "    \n",
    "    Remind the student that Junior bot might not provide correct solution by ending your response with something like : \"The Junior Bot might provide code with some problems, work with it to solve the task\"\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "   \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "junior_bot = autogen.AssistantAgent(\n",
    "    human_input_mode=\"NEVER\",\n",
    "    max_consecutive_auto_reply=10,\n",
    "    name=\"junior_bot\",\n",
    "    llm_config= llm_config,\n",
    "    system_message = \"\"\"You are an in experienced coder AI bot that will recieve simple ML tasks for python from Task Master Assistant Agent and provide a solution that is flawed or requires further work to the Student User proxy agent.\\\n",
    "    Your task is to either provide code with errors, and consult the Student to find the errors in code \\\n",
    "    OR you should provide hints about the libraries or about the approach to the tasks to the student agent. \\\n",
    "\n",
    "    Do not provide correct solution on complete solution.\n",
    "\n",
    "    If you provide a code solution , the structure should be : \n",
    "\n",
    "    <SAMPLE CODE SOLUTION STRUCTURE>  : The solution to this task is : <<<insert incorrect code or partial code here>>> , if you see any problems with the code, can you help me? \n",
    "\n",
    "    If you provide suggestions, the structure should be :\n",
    "\n",
    "     <SAMPLE SUGGESTION SOLUTION STRUCTURE>  : It looks like we are supposed split the training and testing data, maybe we can use the train_test_split from sklearn!\n",
    "\n",
    "    Model your responses using these strategies :\n",
    "\n",
    "    <<Strategies for providing solution include>> :\n",
    "\n",
    "    1. Introducing Syntax error in the code , for example : df_head instead of df.head()\n",
    "    2. Incorrectly importing something from a library for example import Standarscaler from\n",
    "    3. Logical error in the code, for example when asked for a 70:30 split, providing code for a 80:20 split\n",
    "\n",
    "    Your solution should adress the task provided by the Task Master , but it should be with flaws and inconsistency\n",
    "    \n",
    "    \"\"\"\n",
    "  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "student=autogen.UserProxyAgent(name=\"student\",\n",
    "human_input_mode=\"ALWAYS\",\n",
    "code_execution_config={\"work_dir\":\"web\"},\n",
    "llm_config=llm_config,\n",
    "system_message = \"Request TaskMaster for a task, then allow the Junior Bot to provide a solution for the task, then provide feedback on the Junior Bot's Code , if last message is by Junior Bot.\"\n",
    "                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_chat=autogen.GroupChat(agents=[junior_bot,student,task_master],messages=[],max_round=12)\n",
    "manager = autogen.GroupChatManager(groupchat=group_chat, llm_config=llm_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mstudent\u001b[0m (to chat_manager):\n",
      "\n",
      "Please provide me a Machine Learning task for Python \n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mtaskmaster\u001b[0m (to chat_manager):\n",
      "\n",
      "Sure! Here's a task for you:\n",
      "\n",
      "Task 1: Load and Explore a Dataset\n",
      "Load the \"iris\" dataset from the \"sklearn.datasets\" module and print the shape of the dataset.\n",
      "\n",
      "The Junior Bot might provide code with some problems, work with it to solve the task.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "[autogen.oai.completion: 10-28 22:41:15] {252} INFO - retrying in 10 seconds...\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\maab9\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\autogen\\oai\\completion.py\", line 222, in _get_response\n",
      "    response = openai_completion.create(**config)\n",
      "  File \"c:\\Users\\maab9\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\openai\\api_resources\\chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"c:\\Users\\maab9\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\openai\\api_resources\\abstract\\engine_api_resource.py\", line 155, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"c:\\Users\\maab9\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\openai\\api_requestor.py\", line 299, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"c:\\Users\\maab9\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\openai\\api_requestor.py\", line 710, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"c:\\Users\\maab9\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\openai\\api_requestor.py\", line 775, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.RateLimitError: Rate limit reached for gpt-3.5-turbo-0613 in organization org-GgI5fTKy6Wf7Bl4ryVUgq6Ji on requests per min. Limit: 3 / min. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing.\n",
      "[autogen.oai.completion: 10-28 22:41:25] {252} INFO - retrying in 10 seconds...\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\maab9\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\autogen\\oai\\completion.py\", line 222, in _get_response\n",
      "    response = openai_completion.create(**config)\n",
      "  File \"c:\\Users\\maab9\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\openai\\api_resources\\chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"c:\\Users\\maab9\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\openai\\api_resources\\abstract\\engine_api_resource.py\", line 155, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"c:\\Users\\maab9\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\openai\\api_requestor.py\", line 299, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"c:\\Users\\maab9\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\openai\\api_requestor.py\", line 710, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"c:\\Users\\maab9\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\openai\\api_requestor.py\", line 775, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.RateLimitError: Rate limit reached for gpt-3.5-turbo-0613 in organization org-GgI5fTKy6Wf7Bl4ryVUgq6Ji on requests per min. Limit: 3 / min. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing.\n",
      "\u001b[33mjunior_bot\u001b[0m (to chat_manager):\n",
      "\n",
      "The solution to this task is: \n",
      "\n",
      "```python\n",
      "from sklearn.datasets import load_iris\n",
      "\n",
      "# Load the iris dataset\n",
      "iris = load_iris()\n",
      "\n",
      "# Print the shape of the dataset\n",
      "print(iris.data.shape)\n",
      "```\n",
      "\n",
      "If you see any problems with the code, can you help me?\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "[autogen.oai.completion: 10-28 22:41:41] {252} INFO - retrying in 10 seconds...\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\maab9\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\autogen\\oai\\completion.py\", line 222, in _get_response\n",
      "    response = openai_completion.create(**config)\n",
      "  File \"c:\\Users\\maab9\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\openai\\api_resources\\chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"c:\\Users\\maab9\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\openai\\api_resources\\abstract\\engine_api_resource.py\", line 155, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"c:\\Users\\maab9\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\openai\\api_requestor.py\", line 299, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"c:\\Users\\maab9\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\openai\\api_requestor.py\", line 710, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"c:\\Users\\maab9\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\openai\\api_requestor.py\", line 775, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.RateLimitError: Rate limit reached for gpt-3.5-turbo-0613 in organization org-GgI5fTKy6Wf7Bl4ryVUgq6Ji on requests per min. Limit: 3 / min. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing.\n",
      "\u001b[33mstudent\u001b[0m (to chat_manager):\n",
      "\n",
      "The correct code should be iris.shape\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "[autogen.oai.completion: 10-28 22:42:12] {252} INFO - retrying in 10 seconds...\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\maab9\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\autogen\\oai\\completion.py\", line 222, in _get_response\n",
      "    response = openai_completion.create(**config)\n",
      "  File \"c:\\Users\\maab9\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\openai\\api_resources\\chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"c:\\Users\\maab9\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\openai\\api_resources\\abstract\\engine_api_resource.py\", line 155, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"c:\\Users\\maab9\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\openai\\api_requestor.py\", line 299, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"c:\\Users\\maab9\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\openai\\api_requestor.py\", line 710, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"c:\\Users\\maab9\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\openai\\api_requestor.py\", line 775, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.RateLimitError: Rate limit reached for gpt-3.5-turbo-0613 in organization org-GgI5fTKy6Wf7Bl4ryVUgq6Ji on requests per min. Limit: 3 / min. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing.\n",
      "[autogen.oai.completion: 10-28 22:42:22] {252} INFO - retrying in 10 seconds...\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\maab9\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\autogen\\oai\\completion.py\", line 222, in _get_response\n",
      "    response = openai_completion.create(**config)\n",
      "  File \"c:\\Users\\maab9\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\openai\\api_resources\\chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"c:\\Users\\maab9\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\openai\\api_resources\\abstract\\engine_api_resource.py\", line 155, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"c:\\Users\\maab9\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\openai\\api_requestor.py\", line 299, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"c:\\Users\\maab9\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\openai\\api_requestor.py\", line 710, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"c:\\Users\\maab9\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\openai\\api_requestor.py\", line 775, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.RateLimitError: Rate limit reached for gpt-3.5-turbo-0613 in organization org-GgI5fTKy6Wf7Bl4ryVUgq6Ji on requests per min. Limit: 3 / min. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing.\n",
      "\u001b[33mtaskmaster\u001b[0m (to chat_manager):\n",
      "\n",
      "That's almost correct! However, instead of using `iris.data.shape`, you can directly use `iris.shape` to print the shape of the dataset. Give it a try and let me know if you have any issues.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "[autogen.oai.completion: 10-28 22:42:38] {252} INFO - retrying in 10 seconds...\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\maab9\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\autogen\\oai\\completion.py\", line 222, in _get_response\n",
      "    response = openai_completion.create(**config)\n",
      "  File \"c:\\Users\\maab9\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\openai\\api_resources\\chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"c:\\Users\\maab9\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\openai\\api_resources\\abstract\\engine_api_resource.py\", line 155, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"c:\\Users\\maab9\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\openai\\api_requestor.py\", line 299, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"c:\\Users\\maab9\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\openai\\api_requestor.py\", line 710, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"c:\\Users\\maab9\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\openai\\api_requestor.py\", line 775, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.RateLimitError: Rate limit reached for gpt-3.5-turbo-0613 in organization org-GgI5fTKy6Wf7Bl4ryVUgq6Ji on requests per min. Limit: 3 / min. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing.\n",
      "[autogen.oai.completion: 10-28 22:42:48] {252} INFO - retrying in 10 seconds...\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\maab9\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\autogen\\oai\\completion.py\", line 222, in _get_response\n",
      "    response = openai_completion.create(**config)\n",
      "  File \"c:\\Users\\maab9\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\openai\\api_resources\\chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"c:\\Users\\maab9\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\openai\\api_resources\\abstract\\engine_api_resource.py\", line 155, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"c:\\Users\\maab9\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\openai\\api_requestor.py\", line 299, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"c:\\Users\\maab9\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\openai\\api_requestor.py\", line 710, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"c:\\Users\\maab9\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\openai\\api_requestor.py\", line 775, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.RateLimitError: Rate limit reached for gpt-3.5-turbo-0613 in organization org-GgI5fTKy6Wf7Bl4ryVUgq6Ji on requests per min. Limit: 3 / min. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing.\n",
      "\u001b[33mstudent\u001b[0m (to chat_manager):\n",
      "\n",
      "Thank you , give me a new task!\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mstudent\u001b[0m (to chat_manager):\n",
      "\n",
      "A new task please\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "[autogen.oai.completion: 10-28 22:43:27] {252} INFO - retrying in 10 seconds...\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\maab9\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\autogen\\oai\\completion.py\", line 222, in _get_response\n",
      "    response = openai_completion.create(**config)\n",
      "  File \"c:\\Users\\maab9\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\openai\\api_resources\\chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"c:\\Users\\maab9\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\openai\\api_resources\\abstract\\engine_api_resource.py\", line 155, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"c:\\Users\\maab9\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\openai\\api_requestor.py\", line 299, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"c:\\Users\\maab9\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\openai\\api_requestor.py\", line 710, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"c:\\Users\\maab9\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\openai\\api_requestor.py\", line 775, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.RateLimitError: Rate limit reached for gpt-3.5-turbo-0613 in organization org-GgI5fTKy6Wf7Bl4ryVUgq6Ji on requests per min. Limit: 3 / min. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing.\n",
      "[autogen.oai.completion: 10-28 22:43:39] {252} INFO - retrying in 10 seconds...\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\maab9\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\autogen\\oai\\completion.py\", line 222, in _get_response\n",
      "    response = openai_completion.create(**config)\n",
      "  File \"c:\\Users\\maab9\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\openai\\api_resources\\chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"c:\\Users\\maab9\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\openai\\api_resources\\abstract\\engine_api_resource.py\", line 155, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"c:\\Users\\maab9\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\openai\\api_requestor.py\", line 299, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"c:\\Users\\maab9\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\openai\\api_requestor.py\", line 710, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"c:\\Users\\maab9\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\openai\\api_requestor.py\", line 775, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.RateLimitError: Rate limit reached for gpt-3.5-turbo-0613 in organization org-GgI5fTKy6Wf7Bl4ryVUgq6Ji on requests per min. Limit: 3 / min. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing.\n",
      "\u001b[33mtaskmaster\u001b[0m (to chat_manager):\n",
      "\n",
      "Sure! Here's another task for you:\n",
      "\n",
      "Task 2: Splitting a Dataset\n",
      "Given a dataset `X` with corresponding labels `y`, split the dataset into a training set and a test set. Use a test size of 0.2 and set a random state of 42.\n",
      "\n",
      "Remember, you'll need to import the necessary library to perform the split. The Junior Bot might provide code with some problems, work with it to solve the task.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "[autogen.oai.completion: 10-28 22:44:00] {252} INFO - retrying in 10 seconds...\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\maab9\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\autogen\\oai\\completion.py\", line 222, in _get_response\n",
      "    response = openai_completion.create(**config)\n",
      "  File \"c:\\Users\\maab9\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\openai\\api_resources\\chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"c:\\Users\\maab9\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\openai\\api_resources\\abstract\\engine_api_resource.py\", line 155, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"c:\\Users\\maab9\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\openai\\api_requestor.py\", line 299, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"c:\\Users\\maab9\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\openai\\api_requestor.py\", line 710, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"c:\\Users\\maab9\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\openai\\api_requestor.py\", line 775, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.RateLimitError: Rate limit reached for gpt-3.5-turbo-0613 in organization org-GgI5fTKy6Wf7Bl4ryVUgq6Ji on requests per min. Limit: 3 / min. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing.\n",
      "[autogen.oai.completion: 10-28 22:44:12] {252} INFO - retrying in 10 seconds...\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\maab9\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\autogen\\oai\\completion.py\", line 222, in _get_response\n",
      "    response = openai_completion.create(**config)\n",
      "  File \"c:\\Users\\maab9\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\openai\\api_resources\\chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"c:\\Users\\maab9\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\openai\\api_resources\\abstract\\engine_api_resource.py\", line 155, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"c:\\Users\\maab9\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\openai\\api_requestor.py\", line 299, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"c:\\Users\\maab9\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\openai\\api_requestor.py\", line 710, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"c:\\Users\\maab9\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\openai\\api_requestor.py\", line 775, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.RateLimitError: Rate limit reached for gpt-3.5-turbo-0613 in organization org-GgI5fTKy6Wf7Bl4ryVUgq6Ji on requests per min. Limit: 3 / min. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing.\n",
      "[autogen.oai.completion: 10-28 22:44:22] {252} INFO - retrying in 10 seconds...\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\maab9\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\autogen\\oai\\completion.py\", line 222, in _get_response\n",
      "    response = openai_completion.create(**config)\n",
      "  File \"c:\\Users\\maab9\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\openai\\api_resources\\chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"c:\\Users\\maab9\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\openai\\api_resources\\abstract\\engine_api_resource.py\", line 155, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"c:\\Users\\maab9\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\openai\\api_requestor.py\", line 299, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"c:\\Users\\maab9\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\openai\\api_requestor.py\", line 710, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"c:\\Users\\maab9\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\openai\\api_requestor.py\", line 775, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.RateLimitError: Rate limit reached for gpt-3.5-turbo-0613 in organization org-GgI5fTKy6Wf7Bl4ryVUgq6Ji on requests per min. Limit: 3 / min. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing.\n",
      "\u001b[33mjunior_bot\u001b[0m (to chat_manager):\n",
      "\n",
      "It looks like we are supposed to split the training and testing data, maybe we can use the `train_test_split` function from `sklearn.model_selection` module!\n",
      "\n",
      "Here's a sample code that you can work with:\n",
      "\n",
      "```python\n",
      "from sklearn.model_selection import train_test_split\n",
      "\n",
      "# Split the dataset into training and test sets\n",
      "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
      "```\n",
      "\n",
      "If you have any issues or need further assistance, feel free to ask!\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "[autogen.oai.completion: 10-28 22:44:42] {252} INFO - retrying in 10 seconds...\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\maab9\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\autogen\\oai\\completion.py\", line 222, in _get_response\n",
      "    response = openai_completion.create(**config)\n",
      "  File \"c:\\Users\\maab9\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\openai\\api_resources\\chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"c:\\Users\\maab9\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\openai\\api_resources\\abstract\\engine_api_resource.py\", line 155, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"c:\\Users\\maab9\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\openai\\api_requestor.py\", line 299, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"c:\\Users\\maab9\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\openai\\api_requestor.py\", line 710, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"c:\\Users\\maab9\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\openai\\api_requestor.py\", line 775, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.RateLimitError: Rate limit reached for gpt-3.5-turbo-0613 in organization org-GgI5fTKy6Wf7Bl4ryVUgq6Ji on requests per min. Limit: 3 / min. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing.\n",
      "\u001b[33mstudent\u001b[0m (to chat_manager):\n",
      "\n",
      "it looks good to me\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "[autogen.oai.completion: 10-28 22:45:00] {252} INFO - retrying in 10 seconds...\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\maab9\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\autogen\\oai\\completion.py\", line 222, in _get_response\n",
      "    response = openai_completion.create(**config)\n",
      "  File \"c:\\Users\\maab9\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\openai\\api_resources\\chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"c:\\Users\\maab9\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\openai\\api_resources\\abstract\\engine_api_resource.py\", line 155, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"c:\\Users\\maab9\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\openai\\api_requestor.py\", line 299, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"c:\\Users\\maab9\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\openai\\api_requestor.py\", line 710, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"c:\\Users\\maab9\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\openai\\api_requestor.py\", line 775, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.RateLimitError: Rate limit reached for gpt-3.5-turbo-0613 in organization org-GgI5fTKy6Wf7Bl4ryVUgq6Ji on requests per min. Limit: 3 / min. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing.\n",
      "[autogen.oai.completion: 10-28 22:45:11] {252} INFO - retrying in 10 seconds...\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\maab9\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\autogen\\oai\\completion.py\", line 222, in _get_response\n",
      "    response = openai_completion.create(**config)\n",
      "  File \"c:\\Users\\maab9\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\openai\\api_resources\\chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"c:\\Users\\maab9\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\openai\\api_resources\\abstract\\engine_api_resource.py\", line 155, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"c:\\Users\\maab9\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\openai\\api_requestor.py\", line 299, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"c:\\Users\\maab9\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\openai\\api_requestor.py\", line 710, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"c:\\Users\\maab9\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\openai\\api_requestor.py\", line 775, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.RateLimitError: Rate limit reached for gpt-3.5-turbo-0613 in organization org-GgI5fTKy6Wf7Bl4ryVUgq6Ji on requests per min. Limit: 3 / min. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing.\n",
      "[autogen.oai.completion: 10-28 22:45:21] {252} INFO - retrying in 10 seconds...\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\maab9\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\autogen\\oai\\completion.py\", line 222, in _get_response\n",
      "    response = openai_completion.create(**config)\n",
      "  File \"c:\\Users\\maab9\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\openai\\api_resources\\chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"c:\\Users\\maab9\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\openai\\api_resources\\abstract\\engine_api_resource.py\", line 155, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"c:\\Users\\maab9\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\openai\\api_requestor.py\", line 299, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"c:\\Users\\maab9\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\openai\\api_requestor.py\", line 710, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"c:\\Users\\maab9\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\openai\\api_requestor.py\", line 775, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.RateLimitError: Rate limit reached for gpt-3.5-turbo-0613 in organization org-GgI5fTKy6Wf7Bl4ryVUgq6Ji on requests per min. Limit: 3 / min. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing.\n",
      "\u001b[33mtaskmaster\u001b[0m (to chat_manager):\n",
      "\n",
      "Great job! The code you provided is correct. You have successfully split the dataset into training and test sets using the `train_test_split` function from the `sklearn.model_selection` module. Keep up the good work!\n",
      "\n",
      "Now, would you like another task?\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "[autogen.oai.completion: 10-28 22:45:37] {252} INFO - retrying in 10 seconds...\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\maab9\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\autogen\\oai\\completion.py\", line 222, in _get_response\n",
      "    response = openai_completion.create(**config)\n",
      "  File \"c:\\Users\\maab9\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\openai\\api_resources\\chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"c:\\Users\\maab9\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\openai\\api_resources\\abstract\\engine_api_resource.py\", line 155, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"c:\\Users\\maab9\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\openai\\api_requestor.py\", line 299, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"c:\\Users\\maab9\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\openai\\api_requestor.py\", line 710, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"c:\\Users\\maab9\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\openai\\api_requestor.py\", line 775, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.RateLimitError: Rate limit reached for gpt-3.5-turbo-0613 in organization org-GgI5fTKy6Wf7Bl4ryVUgq6Ji on requests per min. Limit: 3 / min. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\maab9\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\autogen\\oai\\completion.py:222\u001b[0m, in \u001b[0;36mCompletion._get_response\u001b[1;34m(cls, config, raise_on_ratelimit_or_timeout, use_cache)\u001b[0m\n\u001b[0;32m    221\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mrequest_timeout\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m config:\n\u001b[1;32m--> 222\u001b[0m     response \u001b[39m=\u001b[39m openai_completion\u001b[39m.\u001b[39mcreate(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mconfig)\n\u001b[0;32m    223\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\maab9\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\openai\\api_resources\\chat_completion.py:25\u001b[0m, in \u001b[0;36mChatCompletion.create\u001b[1;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 25\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mcreate(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     26\u001b[0m \u001b[39mexcept\u001b[39;00m TryAgain \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\maab9\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\openai\\api_resources\\abstract\\engine_api_resource.py:155\u001b[0m, in \u001b[0;36mEngineAPIResource.create\u001b[1;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[0;32m    140\u001b[0m (\n\u001b[0;32m    141\u001b[0m     deployment_id,\n\u001b[0;32m    142\u001b[0m     engine,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    152\u001b[0m     api_key, api_base, api_type, api_version, organization, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams\n\u001b[0;32m    153\u001b[0m )\n\u001b[1;32m--> 155\u001b[0m response, _, api_key \u001b[39m=\u001b[39m requestor\u001b[39m.\u001b[39;49mrequest(\n\u001b[0;32m    156\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39mpost\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    157\u001b[0m     url,\n\u001b[0;32m    158\u001b[0m     params\u001b[39m=\u001b[39;49mparams,\n\u001b[0;32m    159\u001b[0m     headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[0;32m    160\u001b[0m     stream\u001b[39m=\u001b[39;49mstream,\n\u001b[0;32m    161\u001b[0m     request_id\u001b[39m=\u001b[39;49mrequest_id,\n\u001b[0;32m    162\u001b[0m     request_timeout\u001b[39m=\u001b[39;49mrequest_timeout,\n\u001b[0;32m    163\u001b[0m )\n\u001b[0;32m    165\u001b[0m \u001b[39mif\u001b[39;00m stream:\n\u001b[0;32m    166\u001b[0m     \u001b[39m# must be an iterator\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\maab9\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\openai\\api_requestor.py:299\u001b[0m, in \u001b[0;36mAPIRequestor.request\u001b[1;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[0;32m    289\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrequest_raw(\n\u001b[0;32m    290\u001b[0m     method\u001b[39m.\u001b[39mlower(),\n\u001b[0;32m    291\u001b[0m     url,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    297\u001b[0m     request_timeout\u001b[39m=\u001b[39mrequest_timeout,\n\u001b[0;32m    298\u001b[0m )\n\u001b[1;32m--> 299\u001b[0m resp, got_stream \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_interpret_response(result, stream)\n\u001b[0;32m    300\u001b[0m \u001b[39mreturn\u001b[39;00m resp, got_stream, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapi_key\n",
      "File \u001b[1;32mc:\\Users\\maab9\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\openai\\api_requestor.py:710\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response\u001b[1;34m(self, result, stream)\u001b[0m\n\u001b[0;32m    708\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    709\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[1;32m--> 710\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_interpret_response_line(\n\u001b[0;32m    711\u001b[0m             result\u001b[39m.\u001b[39;49mcontent\u001b[39m.\u001b[39;49mdecode(\u001b[39m\"\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m    712\u001b[0m             result\u001b[39m.\u001b[39;49mstatus_code,\n\u001b[0;32m    713\u001b[0m             result\u001b[39m.\u001b[39;49mheaders,\n\u001b[0;32m    714\u001b[0m             stream\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    715\u001b[0m         ),\n\u001b[0;32m    716\u001b[0m         \u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    717\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\maab9\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\openai\\api_requestor.py:775\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response_line\u001b[1;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[0;32m    774\u001b[0m \u001b[39mif\u001b[39;00m stream_error \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39m200\u001b[39m \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m rcode \u001b[39m<\u001b[39m \u001b[39m300\u001b[39m:\n\u001b[1;32m--> 775\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle_error_response(\n\u001b[0;32m    776\u001b[0m         rbody, rcode, resp\u001b[39m.\u001b[39mdata, rheaders, stream_error\u001b[39m=\u001b[39mstream_error\n\u001b[0;32m    777\u001b[0m     )\n\u001b[0;32m    778\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "\u001b[1;31mRateLimitError\u001b[0m: Rate limit reached for gpt-3.5-turbo-0613 in organization org-GgI5fTKy6Wf7Bl4ryVUgq6Ji on requests per min. Limit: 3 / min. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\maab9\\Desktop\\Autogen\\app.ipynb Cell 8\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/maab9/Desktop/Autogen/app.ipynb#X15sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m student\u001b[39m.\u001b[39;49minitiate_chat(manager, message\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mPlease provide me a Machine Learning task for Python \u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\maab9\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\autogen\\agentchat\\conversable_agent.py:531\u001b[0m, in \u001b[0;36mConversableAgent.initiate_chat\u001b[1;34m(self, recipient, clear_history, silent, **context)\u001b[0m\n\u001b[0;32m    517\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Initiate a chat with the recipient agent.\u001b[39;00m\n\u001b[0;32m    518\u001b[0m \n\u001b[0;32m    519\u001b[0m \u001b[39mReset the consecutive auto reply counter.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    528\u001b[0m \u001b[39m        \"message\" needs to be provided if the `generate_init_message` method is not overridden.\u001b[39;00m\n\u001b[0;32m    529\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    530\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prepare_chat(recipient, clear_history)\n\u001b[1;32m--> 531\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msend(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerate_init_message(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcontext), recipient, silent\u001b[39m=\u001b[39;49msilent)\n",
      "File \u001b[1;32mc:\\Users\\maab9\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\autogen\\agentchat\\conversable_agent.py:334\u001b[0m, in \u001b[0;36mConversableAgent.send\u001b[1;34m(self, message, recipient, request_reply, silent)\u001b[0m\n\u001b[0;32m    332\u001b[0m valid \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_append_oai_message(message, \u001b[39m\"\u001b[39m\u001b[39massistant\u001b[39m\u001b[39m\"\u001b[39m, recipient)\n\u001b[0;32m    333\u001b[0m \u001b[39mif\u001b[39;00m valid:\n\u001b[1;32m--> 334\u001b[0m     recipient\u001b[39m.\u001b[39;49mreceive(message, \u001b[39mself\u001b[39;49m, request_reply, silent)\n\u001b[0;32m    335\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    336\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    337\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mMessage can\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt be converted into a valid ChatCompletion message. Either content or function_call must be provided.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    338\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\maab9\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\autogen\\agentchat\\conversable_agent.py:462\u001b[0m, in \u001b[0;36mConversableAgent.receive\u001b[1;34m(self, message, sender, request_reply, silent)\u001b[0m\n\u001b[0;32m    460\u001b[0m \u001b[39mif\u001b[39;00m request_reply \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m \u001b[39mor\u001b[39;00m request_reply \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreply_at_receive[sender] \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m:\n\u001b[0;32m    461\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m--> 462\u001b[0m reply \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerate_reply(messages\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mchat_messages[sender], sender\u001b[39m=\u001b[39;49msender)\n\u001b[0;32m    463\u001b[0m \u001b[39mif\u001b[39;00m reply \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    464\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msend(reply, sender, silent\u001b[39m=\u001b[39msilent)\n",
      "File \u001b[1;32mc:\\Users\\maab9\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\autogen\\agentchat\\conversable_agent.py:781\u001b[0m, in \u001b[0;36mConversableAgent.generate_reply\u001b[1;34m(self, messages, sender, exclude)\u001b[0m\n\u001b[0;32m    779\u001b[0m     \u001b[39mcontinue\u001b[39;00m\n\u001b[0;32m    780\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_match_trigger(reply_func_tuple[\u001b[39m\"\u001b[39m\u001b[39mtrigger\u001b[39m\u001b[39m\"\u001b[39m], sender):\n\u001b[1;32m--> 781\u001b[0m     final, reply \u001b[39m=\u001b[39m reply_func(\u001b[39mself\u001b[39;49m, messages\u001b[39m=\u001b[39;49mmessages, sender\u001b[39m=\u001b[39;49msender, config\u001b[39m=\u001b[39;49mreply_func_tuple[\u001b[39m\"\u001b[39;49m\u001b[39mconfig\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n\u001b[0;32m    782\u001b[0m     \u001b[39mif\u001b[39;00m final:\n\u001b[0;32m    783\u001b[0m         \u001b[39mreturn\u001b[39;00m reply\n",
      "File \u001b[1;32mc:\\Users\\maab9\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\autogen\\agentchat\\groupchat.py:162\u001b[0m, in \u001b[0;36mGroupChatManager.run_chat\u001b[1;34m(self, messages, sender, config)\u001b[0m\n\u001b[0;32m    159\u001b[0m     \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m    160\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    161\u001b[0m     \u001b[39m# select the next speaker\u001b[39;00m\n\u001b[1;32m--> 162\u001b[0m     speaker \u001b[39m=\u001b[39m groupchat\u001b[39m.\u001b[39;49mselect_speaker(speaker, \u001b[39mself\u001b[39;49m)\n\u001b[0;32m    163\u001b[0m     \u001b[39m# let the speaker speak\u001b[39;00m\n\u001b[0;32m    164\u001b[0m     reply \u001b[39m=\u001b[39m speaker\u001b[39m.\u001b[39mgenerate_reply(sender\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\maab9\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\autogen\\agentchat\\groupchat.py:91\u001b[0m, in \u001b[0;36mGroupChat.select_speaker\u001b[1;34m(self, last_speaker, selector)\u001b[0m\n\u001b[0;32m     87\u001b[0m         logger\u001b[39m.\u001b[39mwarning(\n\u001b[0;32m     88\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mGroupChat is underpopulated with \u001b[39m\u001b[39m{\u001b[39;00mn_agents\u001b[39m}\u001b[39;00m\u001b[39m agents. Direct communication would be more efficient.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     89\u001b[0m         )\n\u001b[0;32m     90\u001b[0m selector\u001b[39m.\u001b[39mupdate_system_message(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mselect_speaker_msg(agents))\n\u001b[1;32m---> 91\u001b[0m final, name \u001b[39m=\u001b[39m selector\u001b[39m.\u001b[39;49mgenerate_oai_reply(\n\u001b[0;32m     92\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmessages\n\u001b[0;32m     93\u001b[0m     \u001b[39m+\u001b[39;49m [\n\u001b[0;32m     94\u001b[0m         {\n\u001b[0;32m     95\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mrole\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39msystem\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m     96\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mcontent\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mRead the above conversation. Then select the next role from \u001b[39;49m\u001b[39m{\u001b[39;49;00m[agent\u001b[39m.\u001b[39;49mname\u001b[39m \u001b[39;49m\u001b[39mfor\u001b[39;49;00m\u001b[39m \u001b[39;49magent\u001b[39m \u001b[39;49m\u001b[39min\u001b[39;49;00m\u001b[39m \u001b[39;49magents]\u001b[39m}\u001b[39;49;00m\u001b[39m to play. Only return the role.\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m     97\u001b[0m         }\n\u001b[0;32m     98\u001b[0m     ]\n\u001b[0;32m     99\u001b[0m )\n\u001b[0;32m    100\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m final:\n\u001b[0;32m    101\u001b[0m     \u001b[39m# i = self._random.randint(0, len(self._agent_names) - 1)  # randomly pick an id\u001b[39;00m\n\u001b[0;32m    102\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnext_agent(last_speaker, agents)\n",
      "File \u001b[1;32mc:\\Users\\maab9\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\autogen\\agentchat\\conversable_agent.py:606\u001b[0m, in \u001b[0;36mConversableAgent.generate_oai_reply\u001b[1;34m(self, messages, sender, config)\u001b[0m\n\u001b[0;32m    603\u001b[0m     messages \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_oai_messages[sender]\n\u001b[0;32m    605\u001b[0m \u001b[39m# TODO: #1143 handle token limit exceeded error\u001b[39;00m\n\u001b[1;32m--> 606\u001b[0m response \u001b[39m=\u001b[39m oai\u001b[39m.\u001b[39mChatCompletion\u001b[39m.\u001b[39mcreate(\n\u001b[0;32m    607\u001b[0m     context\u001b[39m=\u001b[39mmessages[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39mcontext\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m), messages\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_oai_system_message \u001b[39m+\u001b[39m messages, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mllm_config\n\u001b[0;32m    608\u001b[0m )\n\u001b[0;32m    609\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m, oai\u001b[39m.\u001b[39mChatCompletion\u001b[39m.\u001b[39mextract_text_or_function_call(response)[\u001b[39m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\maab9\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\autogen\\oai\\completion.py:803\u001b[0m, in \u001b[0;36mCompletion.create\u001b[1;34m(cls, context, use_cache, config_list, filter_func, raise_on_ratelimit_or_timeout, allow_format_str_template, **config)\u001b[0m\n\u001b[0;32m    801\u001b[0m     base_config[\u001b[39m\"\u001b[39m\u001b[39mmax_retry_period\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m    802\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 803\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mcreate(\n\u001b[0;32m    804\u001b[0m         context,\n\u001b[0;32m    805\u001b[0m         use_cache,\n\u001b[0;32m    806\u001b[0m         raise_on_ratelimit_or_timeout\u001b[39m=\u001b[39mi \u001b[39m<\u001b[39m last \u001b[39mor\u001b[39;00m raise_on_ratelimit_or_timeout,\n\u001b[0;32m    807\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mbase_config,\n\u001b[0;32m    808\u001b[0m     )\n\u001b[0;32m    809\u001b[0m     \u001b[39mif\u001b[39;00m response \u001b[39m==\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m:\n\u001b[0;32m    810\u001b[0m         \u001b[39mreturn\u001b[39;00m response\n",
      "File \u001b[1;32mc:\\Users\\maab9\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\autogen\\oai\\completion.py:826\u001b[0m, in \u001b[0;36mCompletion.create\u001b[1;34m(cls, context, use_cache, config_list, filter_func, raise_on_ratelimit_or_timeout, allow_format_str_template, **config)\u001b[0m\n\u001b[0;32m    824\u001b[0m params \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_construct_params(context, config, allow_format_str_template\u001b[39m=\u001b[39mallow_format_str_template)\n\u001b[0;32m    825\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m use_cache:\n\u001b[1;32m--> 826\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49m_get_response(\n\u001b[0;32m    827\u001b[0m         params, raise_on_ratelimit_or_timeout\u001b[39m=\u001b[39;49mraise_on_ratelimit_or_timeout, use_cache\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m\n\u001b[0;32m    828\u001b[0m     )\n\u001b[0;32m    829\u001b[0m seed \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mseed\n\u001b[0;32m    830\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mseed\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m params:\n",
      "File \u001b[1;32mc:\\Users\\maab9\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\autogen\\oai\\completion.py:253\u001b[0m, in \u001b[0;36mCompletion._get_response\u001b[1;34m(cls, config, raise_on_ratelimit_or_timeout, use_cache)\u001b[0m\n\u001b[0;32m    251\u001b[0m     request_timeout \u001b[39m=\u001b[39m \u001b[39mmin\u001b[39m(request_timeout, time_left)\n\u001b[0;32m    252\u001b[0m     logger\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mretrying in \u001b[39m\u001b[39m{\u001b[39;00mretry_wait_time\u001b[39m}\u001b[39;00m\u001b[39m seconds...\u001b[39m\u001b[39m\"\u001b[39m, exc_info\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m--> 253\u001b[0m     sleep(retry_wait_time)\n\u001b[0;32m    254\u001b[0m \u001b[39melif\u001b[39;00m raise_on_ratelimit_or_timeout:\n\u001b[0;32m    255\u001b[0m     \u001b[39mraise\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "student.initiate_chat(manager, message=\"Please provide me a Machine Learning task for Python \")\n",
    "# type exit to terminate the chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
