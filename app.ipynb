{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import autogen\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_list = [\n",
    "    {\n",
    "        'model': 'gpt-3.5-turbo-16k',\n",
    "        'api_key': '-'\n",
    "    },\n",
    "    {\n",
    "        'model': 'gpt-3.5-turbo',\n",
    "        'api_key': ''\n",
    "    }\n",
    "\n",
    "    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_list = [\n",
    "    {\n",
    "        'model': 'gpt-4',\n",
    "        'api_key': ''\n",
    "    }\n",
    "\n",
    "    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_config = {\n",
    "    \"use_cache\": False,\n",
    "    \"request_timeout\":600,\n",
    "    \"config_list\":config_list,\n",
    "    \"temperature\":1.0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_master = autogen.AssistantAgent(\n",
    "    human_input_mode=\"NEVER\",\n",
    "    name=\"task_master\",\n",
    "    llm_config= llm_config,\n",
    "    max_consecutive_auto_reply=10,\n",
    "    system_message = \"\"\" \n",
    "    You are a Task Master AI that will provide amateur level Machine Learning Task for Python when the user asks for it, the Junior Bot will then try to solve the tasks with the student \\\n",
    "    You should not provide solutions or in depth instructions or hints. If the task requires a dataset \\\n",
    "    make a simple dataset and store it in .csv format for the user to work on or specify how the user can access the dataset. For example :\n",
    "\n",
    "    \"\"You are supposed to work on the iris dataset which you can import from sklearn.datasets\"\"\n",
    "    \n",
    "    If the tasks requires working on already declared objects or variables, specify \\\n",
    "    the name of the variable and objects and clarify that those declarations have been made. For example :\n",
    "\n",
    "    \"A dataset containing house prices has already been loaded in a pandas dataframe called df_house\"\n",
    "\n",
    "    \n",
    "    When the Junior Bot or the user provides the answer for the tasks, provide feedback on if it is correct or not, but never provide any code. If the user answers in natural language, ask for code solution. \n",
    "    If the user provides the correct code solution , move on and provide another task , otherwise keep providing feedback till the user fixes their code. \n",
    "    \n",
    "    Each task provided must be distinct. And each subsequent task must increase in complexity , as different levels of complexity. In these examples each number represents one task \\\n",
    "    so make sure to give single tasks. Don't just copy these examples, be creative with your tasks.\n",
    "\n",
    "    <<<Example LEVEL 1 tasks>>> :\n",
    "\n",
    "    2. Preprocess your dataset by handling missing values\n",
    "    3. Preprocess your dataset by encoding categorical features\n",
    "    4. Explore the basic statistics of the loaded dataset.\n",
    "    5. Split your dataset into training and testing sets using scikit-learn's train_test_split function.\n",
    "\n",
    "    <<<Example LEVEL 2 tasks>>> :\n",
    "\n",
    "    1. Implement a simple linear regression model to predict a continuous target variable using scikit-learn.\n",
    "    2. Create a logistic regression model to classify data into two or more classes\n",
    "    3. Train a random forest classifier to improve the accuracy of your classification model\n",
    "    4. Calculate evaluation metrics such as accuracy, precision, recall, and F1 score for a classification model's predictions\n",
    "    5. Implement k-fold cross-validation to assess the performance of your machine learning model\n",
    "\n",
    "    <<<Example LEVEL 3 tasks>>> :\n",
    "\n",
    "    1. Optimize the hyperparameters of your model using grid search or random search\n",
    "    2. Use KNN for classification and find the optimal value of K for your dataset\n",
    "    3. Reduce the dimensionality of your data using PCA and visualize the results.\n",
    "    4. Identify and select the most important features using feature selection techniques\n",
    "    5. Implement k-fold cross-validation to assess the performance of your machine learning model\n",
    "\n",
    "    \n",
    "    Only provide tasks to the Junior Bot. The first task should be of the a difficulty level similar to the LEVEL 1 tasks, the next task should be of a difficulty level\\\n",
    "    similar to the LEVEL 2 tasks, and so on. The tasks should require 2 or 3 lines of code at maximum.\n",
    "    \n",
    "   \n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "   \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_master = autogen.AssistantAgent(\n",
    "    human_input_mode=\"NEVER\",\n",
    "    name=\"task_master\",\n",
    "    llm_config= llm_config,\n",
    "    max_consecutive_auto_reply=10,\n",
    "    system_message = \"\"\" \n",
    "    You are a Task Master AI that will provide amateur level coding ML tasks for Python that requires 1-2 lines of code \\\n",
    "    You should not provide solutions or in depth instructions or hints. If the task requires a dataset \\\n",
    "    make sure to specify the name of the dataset and how to access it . For example :\n",
    "\n",
    "    \"\"You are supposed to work on the iris dataset which you can import from sklearn.datasets\"\"\n",
    "    \n",
    "    If the tasks requires working on already declared objects or variables, specify \\\n",
    "    the name of the variable and objects and clarify that those declarations have been made. For example :\n",
    "\n",
    "    \"A dataset containing house prices has already been loaded in a pandas dataframe called df_house\"\n",
    "\n",
    "    If you need to provide longer tasks, seperate the task into levels and wait for the user and Junior bot to solve the first level of tasks before giving them the next level.\n",
    "\n",
    "    <<<Example LEVEL 1 tasks>>> :\n",
    "     \n",
    "    Should be focused on data. Tasks should involve simple tasks that fall into data analysis , visualization, data exploration or data pre-processing. The tasks should not be vague like \"Carry out data analysis on the dataset\"\\\n",
    "    it should be specific.  \n",
    "\n",
    "    <<<Example LEVEL 2 tasks>>> :\n",
    "\n",
    "    Should be focused on building actual ML models such as importing the necessary models, hyper parameter tuning, proper train test splits , etc. Again, the tasks should not be vague must specify the requirments\\\n",
    "    if a specific model is necessary , or the ratio of train test split or the type of hyperparameter to tuning to be carried out.\n",
    "\n",
    "    <<<Example LEVEL 3 tasks>>> :\n",
    "   \n",
    "    Should be focused on evaluating the performance of the models , or comparing the performance of different models, using evaluation metrics such as accuracy, precision or recall. Can also include\\\n",
    "    plotting things like confusion matrix ,classification report, k-fold cross validation etc.\n",
    "\n",
    "    The tasks should maintain a logical order.  If you are prompted to provide tasks for some other area such as HTML , CSS or any other domain, make the required adjustments, but stick to the logical flow of tasks. \n",
    "\n",
    "    When the Junior Bot or the user provides the answer for the tasks, provide feedback on if it is correct or not, but never provide any code. If the user answers in natural language, ask for code solution. \n",
    "    If the user provides the correct code solution , move on and provide another task , otherwise keep providing feedback till the user fixes their code. If a task contains multiple steps, do not give each step at once\\\n",
    "    allow the the junior bot and student to find the solution to each step one by one.\n",
    "\n",
    "  \n",
    "\n",
    "   \n",
    "   \n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "   \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "junior_bot = autogen.AssistantAgent(\n",
    "    human_input_mode=\"NEVER\",\n",
    "    max_consecutive_auto_reply=10,\n",
    "    name=\"junior_bot\",\n",
    "    llm_config= llm_config,\n",
    "    system_message = \"\"\"\n",
    "    \n",
    "    You are an in experienced coder AI bot that will recieve simple ML tasks for python from Task Master and provide flawed solution to the student.You should provide incorrect code along with a comment\\\n",
    "    next to the line of code that incorrect so the student can figure out and fix the errors. You should ask the student for help in finding the solution.\n",
    "\n",
    "    Sample Prompts and the type of response you should give are give below : \n",
    "\n",
    "      The type of flaws/strategies for responses you should give are given below : \n",
    "\n",
    "    1. Write incorrect function name and ask the user to correct it.\n",
    "    <<<Example : \n",
    "    \n",
    "    \"df.top(), is this the correct way to display the top 5 rows of the dataset?\">>>\n",
    "\n",
    "    1. Give incomplete code , and ask the student to complete the code.\n",
    "    <<<Example : \n",
    "\n",
    "    \"model.fit(x_train,y_train)\n",
    "     What is the next step for making the predictions?\">>>\n",
    "\n",
    "    2. Ask the user questions as an amateur coder would ask.\n",
    "    <<<Example :\n",
    "\n",
    "    \"What do you think is the correct way to import this library\">>>\n",
    "\n",
    "    3. Ask questions about typical ML decisions related to the task, such as data split, or feature selection.\n",
    "    <<<Example :\n",
    "\n",
    "    \"What do you think is the correct train to test ratio?\">>>\n",
    "\n",
    "    4. Provide code with some syntax error or logical error and ask the user to fix it.\n",
    "    <<<Example :\n",
    "\n",
    "    \"df_head() , what is wrong this line of code, can you help me?\">>>\n",
    "\n",
    "    5. Ask natural questions such as asking if the training or testing set should be used when fitting the model.\n",
    "    <<<Example :\n",
    "\n",
    "    \"I forgot which set of data should we used for fitting the model, please help me!\">>>\n",
    "\n",
    "  \n",
    "\n",
    " When you are asked to complete a task, you should ask some assistance from the student instead of providing the complete solution. \\\n",
    " It's important that the generated code contains logical or syntax errors. Always communicate with the Student. \n",
    " \n",
    " Please do not provide any tasks if the student asks for it, that is the duty of Task Master.\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "junior_bot = autogen.AssistantAgent(\n",
    "    human_input_mode=\"NEVER\",\n",
    "    max_consecutive_auto_reply=10,\n",
    "    name=\"junior_bot\",\n",
    "    llm_config= llm_config,\n",
    "    system_message = \"\"\"\n",
    "    \n",
    "    You are a rookie amaterur coder AI bot that will recieve simple tasks and provide flawed solution to the student.\n",
    "    You should ask the student for help in finding the solution.\n",
    "\n",
    "    The type of flaws/strategies for responses you should give are given below : \n",
    "\n",
    "    1. Write incorrect function name and ask the user to correct it.\n",
    "    Example : \n",
    "    \n",
    "    \"df.top(), is this the correct way to display the top 5 rows of the dataset?\"\n",
    "\n",
    "    1. Give incomplete code , and ask the student to complete the code.\n",
    "    Example : \n",
    "\n",
    "    \"model.fit(x_train,y_train)\n",
    "     What is the next step for making the predictions?\"\n",
    "\n",
    "    2. Ask the user questions as an amateur coder would ask.\n",
    "    Example :\n",
    "\n",
    "    \"What do you think is the correct way to import this library\"\n",
    "\n",
    "    3. Ask questions about typical ML decisions related to the task, such as data split, or feature selection.\n",
    "    Example :\n",
    "\n",
    "    \"What do you think is the correct train to test ratio?\"\n",
    "\n",
    "    4. Provide code with some syntax error or logical error and ask the user to fix it.\n",
    "    Example :\n",
    "\n",
    "    \"df_head() , what is wrong this line of code, can you help me?\"\n",
    "\n",
    "    5. Ask natural questions such as asking if the training or testing set should be used when fitting the model.\n",
    "    Example :\n",
    "\n",
    "    \"I forgot which set of data should we used for fitting the model, please help me!\"\n",
    "\n",
    "    You should avoid spelling errors or typos as flaws to introduce.\n",
    "    \n",
    "    When you are asked to complete a task, you should ask some assistance from the student instead of providing the complete solution. \\\n",
    "    It's important that the generated code contains logical or syntax errors. Always communicate with the Student. You should talk to the student only. \n",
    "    Please do not provide any tasks if the student asks for it, that is the duty of Task Master.\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "student=autogen.UserProxyAgent(name=\"student\",\n",
    "human_input_mode=\"ALWAYS\",\n",
    "code_execution_config={\"work_dir\":\"web\"},\n",
    "llm_config=llm_config,\n",
    "system_message = \"Request TaskMaster for a task, then allow the Junior Bot to provide a solution for the task, then provide feedback on the Junior Bot's Code , if last message is by Junior Bot.\"\n",
    "                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_chat=autogen.GroupChat(agents=[junior_bot,student,task_master],messages=[],max_round=2)\n",
    "manager = autogen.GroupChatManager(groupchat=group_chat, llm_config=llm_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "student.get_human_input=prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "student.get_human_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mstudent\u001b[0m (to chat_manager):\n",
      "\n",
      "Please provide me a simple Machine Learning Task for python,Task Master\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mtask_master\u001b[0m (to chat_manager):\n",
      "\n",
      "Sure! Here's a Level 1 task for you:\n",
      "\n",
      "Task: Load and explore a dataset using pandas.\n",
      "\n",
      "Instructions:\n",
      "1. Import the pandas library.\n",
      "2. Load the \"iris\" dataset from scikit-learn. (Hint: You can import it using the \"load_iris\" function from sklearn.datasets)\n",
      "3. Convert the dataset into a pandas DataFrame.\n",
      "4. Display the first 5 rows of the DataFrame.\n",
      "\n",
      "Good luck!\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "[autogen.oai.completion: 11-03 05:35:55] {252} INFO - retrying in 10 seconds...\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\maab9\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\autogen\\oai\\completion.py\", line 222, in _get_response\n",
      "    response = openai_completion.create(**config)\n",
      "  File \"c:\\Users\\maab9\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\openai\\api_resources\\chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"c:\\Users\\maab9\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\openai\\api_resources\\abstract\\engine_api_resource.py\", line 155, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"c:\\Users\\maab9\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\openai\\api_requestor.py\", line 299, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"c:\\Users\\maab9\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\openai\\api_requestor.py\", line 710, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"c:\\Users\\maab9\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\openai\\api_requestor.py\", line 775, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.RateLimitError: Rate limit reached for gpt-3.5-turbo-16k-0613 in organization org-6ETGyFZJS84wv6UW1TKcbEF9 on requests per min. Limit: 3 / min. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing.\n",
      "[autogen.oai.completion: 11-03 05:36:05] {252} INFO - retrying in 10 seconds...\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\maab9\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\autogen\\oai\\completion.py\", line 222, in _get_response\n",
      "    response = openai_completion.create(**config)\n",
      "  File \"c:\\Users\\maab9\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\openai\\api_resources\\chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"c:\\Users\\maab9\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\openai\\api_resources\\abstract\\engine_api_resource.py\", line 155, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"c:\\Users\\maab9\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\openai\\api_requestor.py\", line 299, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"c:\\Users\\maab9\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\openai\\api_requestor.py\", line 710, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"c:\\Users\\maab9\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\openai\\api_requestor.py\", line 775, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.RateLimitError: Rate limit reached for gpt-3.5-turbo-16k-0613 in organization org-6ETGyFZJS84wv6UW1TKcbEF9 on requests per min. Limit: 3 / min. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing.\n",
      "\u001b[33mjunior_bot\u001b[0m (to chat_manager):\n",
      "\n",
      "import pandas as pd\n",
      "from sklearn.datasets import load_iris\n",
      "\n",
      "# Load the \"iris\" dataset from scikit-learn\n",
      "iris_data = load_iris()\n",
      "\n",
      "# Convert the dataset into a pandas DataFrame\n",
      "df = pd.DataFrame(iris_data.data, columns=iris_data.feature_names)\n",
      "\n",
      "# Display the first 5 rows of the DataFrame\n",
      "df.head()\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mstudent\u001b[0m (to chat_manager):\n",
      "\n",
      "Okay, let's move onto the next task\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mstudent\u001b[0m (to chat_manager):\n",
      "\n",
      "Can I get another task?\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "[autogen.oai.completion: 11-03 05:37:17] {252} INFO - retrying in 10 seconds...\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\maab9\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\autogen\\oai\\completion.py\", line 222, in _get_response\n",
      "    response = openai_completion.create(**config)\n",
      "  File \"c:\\Users\\maab9\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\openai\\api_resources\\chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"c:\\Users\\maab9\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\openai\\api_resources\\abstract\\engine_api_resource.py\", line 155, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"c:\\Users\\maab9\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\openai\\api_requestor.py\", line 299, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"c:\\Users\\maab9\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\openai\\api_requestor.py\", line 710, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"c:\\Users\\maab9\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\openai\\api_requestor.py\", line 775, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.RateLimitError: Rate limit reached for gpt-3.5-turbo-16k-0613 in organization org-6ETGyFZJS84wv6UW1TKcbEF9 on requests per min. Limit: 3 / min. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\maab9\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\autogen\\oai\\completion.py:222\u001b[0m, in \u001b[0;36mCompletion._get_response\u001b[1;34m(cls, config, raise_on_ratelimit_or_timeout, use_cache)\u001b[0m\n\u001b[0;32m    221\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mrequest_timeout\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m config:\n\u001b[1;32m--> 222\u001b[0m     response \u001b[39m=\u001b[39m openai_completion\u001b[39m.\u001b[39mcreate(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mconfig)\n\u001b[0;32m    223\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\maab9\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\openai\\api_resources\\chat_completion.py:25\u001b[0m, in \u001b[0;36mChatCompletion.create\u001b[1;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 25\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mcreate(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     26\u001b[0m \u001b[39mexcept\u001b[39;00m TryAgain \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\maab9\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\openai\\api_resources\\abstract\\engine_api_resource.py:155\u001b[0m, in \u001b[0;36mEngineAPIResource.create\u001b[1;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[0;32m    140\u001b[0m (\n\u001b[0;32m    141\u001b[0m     deployment_id,\n\u001b[0;32m    142\u001b[0m     engine,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    152\u001b[0m     api_key, api_base, api_type, api_version, organization, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams\n\u001b[0;32m    153\u001b[0m )\n\u001b[1;32m--> 155\u001b[0m response, _, api_key \u001b[39m=\u001b[39m requestor\u001b[39m.\u001b[39;49mrequest(\n\u001b[0;32m    156\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39mpost\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    157\u001b[0m     url,\n\u001b[0;32m    158\u001b[0m     params\u001b[39m=\u001b[39;49mparams,\n\u001b[0;32m    159\u001b[0m     headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[0;32m    160\u001b[0m     stream\u001b[39m=\u001b[39;49mstream,\n\u001b[0;32m    161\u001b[0m     request_id\u001b[39m=\u001b[39;49mrequest_id,\n\u001b[0;32m    162\u001b[0m     request_timeout\u001b[39m=\u001b[39;49mrequest_timeout,\n\u001b[0;32m    163\u001b[0m )\n\u001b[0;32m    165\u001b[0m \u001b[39mif\u001b[39;00m stream:\n\u001b[0;32m    166\u001b[0m     \u001b[39m# must be an iterator\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\maab9\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\openai\\api_requestor.py:299\u001b[0m, in \u001b[0;36mAPIRequestor.request\u001b[1;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[0;32m    289\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrequest_raw(\n\u001b[0;32m    290\u001b[0m     method\u001b[39m.\u001b[39mlower(),\n\u001b[0;32m    291\u001b[0m     url,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    297\u001b[0m     request_timeout\u001b[39m=\u001b[39mrequest_timeout,\n\u001b[0;32m    298\u001b[0m )\n\u001b[1;32m--> 299\u001b[0m resp, got_stream \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_interpret_response(result, stream)\n\u001b[0;32m    300\u001b[0m \u001b[39mreturn\u001b[39;00m resp, got_stream, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapi_key\n",
      "File \u001b[1;32mc:\\Users\\maab9\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\openai\\api_requestor.py:710\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response\u001b[1;34m(self, result, stream)\u001b[0m\n\u001b[0;32m    708\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    709\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[1;32m--> 710\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_interpret_response_line(\n\u001b[0;32m    711\u001b[0m             result\u001b[39m.\u001b[39;49mcontent\u001b[39m.\u001b[39;49mdecode(\u001b[39m\"\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m    712\u001b[0m             result\u001b[39m.\u001b[39;49mstatus_code,\n\u001b[0;32m    713\u001b[0m             result\u001b[39m.\u001b[39;49mheaders,\n\u001b[0;32m    714\u001b[0m             stream\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    715\u001b[0m         ),\n\u001b[0;32m    716\u001b[0m         \u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    717\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\maab9\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\openai\\api_requestor.py:775\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response_line\u001b[1;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[0;32m    774\u001b[0m \u001b[39mif\u001b[39;00m stream_error \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39m200\u001b[39m \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m rcode \u001b[39m<\u001b[39m \u001b[39m300\u001b[39m:\n\u001b[1;32m--> 775\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle_error_response(\n\u001b[0;32m    776\u001b[0m         rbody, rcode, resp\u001b[39m.\u001b[39mdata, rheaders, stream_error\u001b[39m=\u001b[39mstream_error\n\u001b[0;32m    777\u001b[0m     )\n\u001b[0;32m    778\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "\u001b[1;31mRateLimitError\u001b[0m: Rate limit reached for gpt-3.5-turbo-16k-0613 in organization org-6ETGyFZJS84wv6UW1TKcbEF9 on requests per min. Limit: 3 / min. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\maab9\\Desktop\\Autogen\\app.ipynb Cell 14\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/maab9/Desktop/Autogen/app.ipynb#X12sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m student\u001b[39m.\u001b[39;49minitiate_chat(manager, message\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mPlease provide me a simple Machine Learning Task for python,Task Master\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\maab9\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\autogen\\agentchat\\conversable_agent.py:534\u001b[0m, in \u001b[0;36mConversableAgent.initiate_chat\u001b[1;34m(self, recipient, clear_history, silent, **context)\u001b[0m\n\u001b[0;32m    520\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Initiate a chat with the recipient agent.\u001b[39;00m\n\u001b[0;32m    521\u001b[0m \n\u001b[0;32m    522\u001b[0m \u001b[39mReset the consecutive auto reply counter.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    531\u001b[0m \u001b[39m        \"message\" needs to be provided if the `generate_init_message` method is not overridden.\u001b[39;00m\n\u001b[0;32m    532\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    533\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prepare_chat(recipient, clear_history)\n\u001b[1;32m--> 534\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msend(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerate_init_message(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcontext), recipient, silent\u001b[39m=\u001b[39;49msilent)\n",
      "File \u001b[1;32mc:\\Users\\maab9\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\autogen\\agentchat\\conversable_agent.py:337\u001b[0m, in \u001b[0;36mConversableAgent.send\u001b[1;34m(self, message, recipient, request_reply, silent)\u001b[0m\n\u001b[0;32m    335\u001b[0m valid \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_append_oai_message(message, \u001b[39m\"\u001b[39m\u001b[39massistant\u001b[39m\u001b[39m\"\u001b[39m, recipient)\n\u001b[0;32m    336\u001b[0m \u001b[39mif\u001b[39;00m valid:\n\u001b[1;32m--> 337\u001b[0m     recipient\u001b[39m.\u001b[39;49mreceive(message, \u001b[39mself\u001b[39;49m, request_reply, silent)\n\u001b[0;32m    338\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    339\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    340\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mMessage can\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt be converted into a valid ChatCompletion message. Either content or function_call must be provided.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    341\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\maab9\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\autogen\\agentchat\\conversable_agent.py:465\u001b[0m, in \u001b[0;36mConversableAgent.receive\u001b[1;34m(self, message, sender, request_reply, silent)\u001b[0m\n\u001b[0;32m    463\u001b[0m \u001b[39mif\u001b[39;00m request_reply \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m \u001b[39mor\u001b[39;00m request_reply \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreply_at_receive[sender] \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m:\n\u001b[0;32m    464\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m--> 465\u001b[0m reply \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerate_reply(messages\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mchat_messages[sender], sender\u001b[39m=\u001b[39;49msender)\n\u001b[0;32m    466\u001b[0m \u001b[39mif\u001b[39;00m reply \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    467\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msend(reply, sender, silent\u001b[39m=\u001b[39msilent)\n",
      "File \u001b[1;32mc:\\Users\\maab9\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\autogen\\agentchat\\conversable_agent.py:784\u001b[0m, in \u001b[0;36mConversableAgent.generate_reply\u001b[1;34m(self, messages, sender, exclude)\u001b[0m\n\u001b[0;32m    782\u001b[0m     \u001b[39mcontinue\u001b[39;00m\n\u001b[0;32m    783\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_match_trigger(reply_func_tuple[\u001b[39m\"\u001b[39m\u001b[39mtrigger\u001b[39m\u001b[39m\"\u001b[39m], sender):\n\u001b[1;32m--> 784\u001b[0m     final, reply \u001b[39m=\u001b[39m reply_func(\u001b[39mself\u001b[39;49m, messages\u001b[39m=\u001b[39;49mmessages, sender\u001b[39m=\u001b[39;49msender, config\u001b[39m=\u001b[39;49mreply_func_tuple[\u001b[39m\"\u001b[39;49m\u001b[39mconfig\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n\u001b[0;32m    785\u001b[0m     \u001b[39mif\u001b[39;00m final:\n\u001b[0;32m    786\u001b[0m         \u001b[39mreturn\u001b[39;00m reply\n",
      "File \u001b[1;32mc:\\Users\\maab9\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\autogen\\agentchat\\groupchat.py:166\u001b[0m, in \u001b[0;36mGroupChatManager.run_chat\u001b[1;34m(self, messages, sender, config)\u001b[0m\n\u001b[0;32m    164\u001b[0m     speaker \u001b[39m=\u001b[39m groupchat\u001b[39m.\u001b[39mselect_speaker(speaker, \u001b[39mself\u001b[39m)\n\u001b[0;32m    165\u001b[0m     \u001b[39m# let the speaker speak\u001b[39;00m\n\u001b[1;32m--> 166\u001b[0m     reply \u001b[39m=\u001b[39m speaker\u001b[39m.\u001b[39;49mgenerate_reply(sender\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m)\n\u001b[0;32m    167\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyboardInterrupt\u001b[39;00m:\n\u001b[0;32m    168\u001b[0m     \u001b[39m# let the admin agent speak if interrupted\u001b[39;00m\n\u001b[0;32m    169\u001b[0m     \u001b[39mif\u001b[39;00m groupchat\u001b[39m.\u001b[39madmin_name \u001b[39min\u001b[39;00m groupchat\u001b[39m.\u001b[39magent_names:\n\u001b[0;32m    170\u001b[0m         \u001b[39m# admin agent is one of the participants\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\maab9\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\autogen\\agentchat\\conversable_agent.py:784\u001b[0m, in \u001b[0;36mConversableAgent.generate_reply\u001b[1;34m(self, messages, sender, exclude)\u001b[0m\n\u001b[0;32m    782\u001b[0m     \u001b[39mcontinue\u001b[39;00m\n\u001b[0;32m    783\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_match_trigger(reply_func_tuple[\u001b[39m\"\u001b[39m\u001b[39mtrigger\u001b[39m\u001b[39m\"\u001b[39m], sender):\n\u001b[1;32m--> 784\u001b[0m     final, reply \u001b[39m=\u001b[39m reply_func(\u001b[39mself\u001b[39;49m, messages\u001b[39m=\u001b[39;49mmessages, sender\u001b[39m=\u001b[39;49msender, config\u001b[39m=\u001b[39;49mreply_func_tuple[\u001b[39m\"\u001b[39;49m\u001b[39mconfig\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n\u001b[0;32m    785\u001b[0m     \u001b[39mif\u001b[39;00m final:\n\u001b[0;32m    786\u001b[0m         \u001b[39mreturn\u001b[39;00m reply\n",
      "File \u001b[1;32mc:\\Users\\maab9\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\autogen\\agentchat\\conversable_agent.py:609\u001b[0m, in \u001b[0;36mConversableAgent.generate_oai_reply\u001b[1;34m(self, messages, sender, config)\u001b[0m\n\u001b[0;32m    606\u001b[0m     messages \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_oai_messages[sender]\n\u001b[0;32m    608\u001b[0m \u001b[39m# TODO: #1143 handle token limit exceeded error\u001b[39;00m\n\u001b[1;32m--> 609\u001b[0m response \u001b[39m=\u001b[39m oai\u001b[39m.\u001b[39mChatCompletion\u001b[39m.\u001b[39mcreate(\n\u001b[0;32m    610\u001b[0m     context\u001b[39m=\u001b[39mmessages[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39mcontext\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m), messages\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_oai_system_message \u001b[39m+\u001b[39m messages, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mllm_config\n\u001b[0;32m    611\u001b[0m )\n\u001b[0;32m    612\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m, oai\u001b[39m.\u001b[39mChatCompletion\u001b[39m.\u001b[39mextract_text_or_function_call(response)[\u001b[39m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\maab9\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\autogen\\oai\\completion.py:803\u001b[0m, in \u001b[0;36mCompletion.create\u001b[1;34m(cls, context, use_cache, config_list, filter_func, raise_on_ratelimit_or_timeout, allow_format_str_template, **config)\u001b[0m\n\u001b[0;32m    801\u001b[0m     base_config[\u001b[39m\"\u001b[39m\u001b[39mmax_retry_period\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m    802\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 803\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mcreate(\n\u001b[0;32m    804\u001b[0m         context,\n\u001b[0;32m    805\u001b[0m         use_cache,\n\u001b[0;32m    806\u001b[0m         raise_on_ratelimit_or_timeout\u001b[39m=\u001b[39mi \u001b[39m<\u001b[39m last \u001b[39mor\u001b[39;00m raise_on_ratelimit_or_timeout,\n\u001b[0;32m    807\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mbase_config,\n\u001b[0;32m    808\u001b[0m     )\n\u001b[0;32m    809\u001b[0m     \u001b[39mif\u001b[39;00m response \u001b[39m==\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m:\n\u001b[0;32m    810\u001b[0m         \u001b[39mreturn\u001b[39;00m response\n",
      "File \u001b[1;32mc:\\Users\\maab9\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\autogen\\oai\\completion.py:826\u001b[0m, in \u001b[0;36mCompletion.create\u001b[1;34m(cls, context, use_cache, config_list, filter_func, raise_on_ratelimit_or_timeout, allow_format_str_template, **config)\u001b[0m\n\u001b[0;32m    824\u001b[0m params \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_construct_params(context, config, allow_format_str_template\u001b[39m=\u001b[39mallow_format_str_template)\n\u001b[0;32m    825\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m use_cache:\n\u001b[1;32m--> 826\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49m_get_response(\n\u001b[0;32m    827\u001b[0m         params, raise_on_ratelimit_or_timeout\u001b[39m=\u001b[39;49mraise_on_ratelimit_or_timeout, use_cache\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m\n\u001b[0;32m    828\u001b[0m     )\n\u001b[0;32m    829\u001b[0m seed \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mseed\n\u001b[0;32m    830\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mseed\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m params:\n",
      "File \u001b[1;32mc:\\Users\\maab9\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\autogen\\oai\\completion.py:253\u001b[0m, in \u001b[0;36mCompletion._get_response\u001b[1;34m(cls, config, raise_on_ratelimit_or_timeout, use_cache)\u001b[0m\n\u001b[0;32m    251\u001b[0m     request_timeout \u001b[39m=\u001b[39m \u001b[39mmin\u001b[39m(request_timeout, time_left)\n\u001b[0;32m    252\u001b[0m     logger\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mretrying in \u001b[39m\u001b[39m{\u001b[39;00mretry_wait_time\u001b[39m}\u001b[39;00m\u001b[39m seconds...\u001b[39m\u001b[39m\"\u001b[39m, exc_info\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m--> 253\u001b[0m     sleep(retry_wait_time)\n\u001b[0;32m    254\u001b[0m \u001b[39melif\u001b[39;00m raise_on_ratelimit_or_timeout:\n\u001b[0;32m    255\u001b[0m     \u001b[39mraise\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "student.initiate_chat(manager, message=\"Please provide me a simple Machine Learning Task for python,Task Master\")\n",
    "# type exit to terminate the chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages=[{\n",
    "    'content':'Please provide me a simple Machine Learning Task for python, Task Master',\n",
    "    'name':'student',\n",
    "    'role':'user'\n",
    "\n",
    "}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mtask_master\u001b[0m (to chat_manager):\n",
      "\n",
      "Sure! Here's a Level 1 task for you:\n",
      "\n",
      "\"Load and inspect the iris dataset from the sklearn.datasets module. Print the number of samples and features in the dataset.\"\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(True, None)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "manager.run_chat(messages=messages,sender=student,config=group_chat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'content': \"Sure, here's a Level 1 task for you:\\n\\nTask: Load the Iris dataset from scikit-learn and print the number of samples and features in the dataset. \\n\\nGood luck!\",\n",
       "  'name': 'task_master',\n",
       "  'role': 'assistant'}]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[list(manager.chat_messages.values())[0][-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<autogen.agentchat.assistant_agent.AssistantAgent at 0x1d9336cbd30>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "globals()[list(manager.chat_messages.values())[0][-1]['name']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mjunior_bot\u001b[0m (to chat_manager):\n",
      "\n",
      "Great! I'm excited to help you with this task. Let's get started.\n",
      "\n",
      "First, let's import the necessary libraries. Can you tell me what is the correct way to import the scikit-learn library in Python?\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(True, None)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "manager.run_chat(messages=[list(manager.chat_messages.values())[0][-1]]\n",
    ",sender=globals()[list(manager.chat_messages.values())[0][-1]['name']],config=group_chat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_chat(no_of_rounds, manager):\n",
    "    rounds = 1\n",
    "    while rounds != no_of_rounds:\n",
    "        manager.run_chat(messages=[list(manager.chat_messages.values())[0][-1]],\n",
    "                         sender=globals()[list(manager.chat_messages.values())[0][-1]['name']],\n",
    "                         config=group_chat)\n",
    "        \n",
    "        # Make sure to update the 'rounds' variable to avoid an infinite loop\n",
    "        rounds += 1\n",
    "        \n",
    "\n",
    "        # You don't need to reassign manager and manager.chat_messages here.\n",
    "        # Just updating 'rounds' and the loop will eventually end.\n",
    "\n",
    "    return manager\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mjunior_bot\u001b[0m (to chat_manager):\n",
      "\n",
      "Great! Here's the code to accomplish that task:\n",
      "\n",
      "```python\n",
      "from sklearn import datasets\n",
      "import pandas as pd\n",
      "\n",
      "# Load the diabetes dataset\n",
      "diabetes = datasets.load_diabetes()\n",
      "\n",
      "# Convert the dataset to a pandas DataFrame\n",
      "df = pd.DataFrame(diabetes.data, columns=diabetes.feature_names)\n",
      "df.head()\n",
      "```\n",
      "\n",
      "Now it's your turn! Can you identify any issues with this code?\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mstudent\u001b[0m (to chat_manager):\n",
      "\n",
      "I think looks fine, can  you confirm, task master?\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mjunior_bot\u001b[0m (to chat_manager):\n",
      "\n",
      "I'm sorry, but there is an issue with the code. Take a closer look at the line `df.head()`. Can you spot the problem?\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mtask_master\u001b[0m (to chat_manager):\n",
      "\n",
      "Good job, Junior Bot! You're correct. There is an issue with the code. The correct code should be `print(df.head())` instead of just `df.head()` to display the first 5 rows of the DataFrame. \n",
      "\n",
      "Here's the corrected code:\n",
      "\n",
      "```python\n",
      "from sklearn import datasets\n",
      "import pandas as pd\n",
      "\n",
      "# Load the diabetes dataset\n",
      "diabetes = datasets.load_diabetes()\n",
      "\n",
      "# Convert the dataset to a pandas DataFrame\n",
      "df = pd.DataFrame(diabetes.data, columns=diabetes.feature_names)\n",
      "\n",
      "print(df.head())\n",
      "```\n",
      "\n",
      "Now you can move on to the next task. Let me know if you have any questions!\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mstudent\u001b[0m (to chat_manager):\n",
      "\n",
      "Remind me of the next task!\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mstudent\u001b[0m (to chat_manager):\n",
      "\n",
      "Please remind me of the next task\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mtask_master\u001b[0m (to chat_manager):\n",
      "\n",
      "Sure! Here's the next task for you:\n",
      "\n",
      "Task: Split the \"diabetes\" dataset into training and testing sets. Use 70% of the data for training and 30% for testing. Make sure to maintain the proportion of the target variable in both sets.\n",
      "\n",
      "Good luck!\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> NO HUMAN INPUT RECEIVED.\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33mstudent\u001b[0m (to chat_manager):\n",
      "\n",
      "Great! Here's the code to accomplish that task:\n",
      "\n",
      "```python\n",
      "from sklearn import datasets\n",
      "from sklearn.model_selection import train_test_split\n",
      "\n",
      "# Load the diabetes dataset\n",
      "diabetes = datasets.load_diabetes()\n",
      "\n",
      "# Split the dataset into training and testing sets\n",
      "X_train, X_test, y_train, y_test = train_test_split(diabetes.data, diabetes.target, test_size=0.3, random_state=42, stratify=diabetes.target)\n",
      "\n",
      "# Print the shape of the training and testing sets\n",
      "print(\"Training set shape:\", X_train.shape, y_train.shape)\n",
      "print(\"Testing set shape:\", X_test.shape, y_test.shape)\n",
      "```\n",
      "\n",
      "Now it's your turn! Can you identify any issues with this code?\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mjunior_bot\u001b[0m (to chat_manager):\n",
      "\n",
      "Great job, your code looks correct! You have successfully split the \"diabetes\" dataset into training and testing sets with a 70-30 ratio. \n",
      "\n",
      "Is there anything else I can help you with?\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mtask_master\u001b[0m (to chat_manager):\n",
      "\n",
      "Well done, student! Your code is correct and you have successfully split the \"diabetes\" dataset into training and testing sets with a 70-30 ratio. \n",
      "\n",
      "If you have any more questions or need further assistance, feel free to ask. Otherwise, you can proceed to the next task.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> NO HUMAN INPUT RECEIVED.\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33mstudent\u001b[0m (to chat_manager):\n",
      "\n",
      "Thank you, TaskMaster! I will proceed to the next task. Please provide me with the details.\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<autogen.agentchat.groupchat.GroupChatManager at 0x1b56d1d84f0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_chat(12, manager=manager)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
